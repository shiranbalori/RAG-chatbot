{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d52b119-5384-401b-99ce-365c5cfe214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "\n",
    "pdf_path = input(\"Please enter the full path to the PDF file (or press Enter to use the default):\\n\").strip()\n",
    "\n",
    "if not pdf_path:\n",
    "    pdf_path = r\"C:\\Users\\shira\\OneDrive\\שולחן העבודה\\BOT_Elad_Sistem\\international_agreements_uae_bit-eng.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(\"❌ File not found. Please check the path and try again.\")\n",
    "    exit(1)\n",
    "\n",
    "API_KEY = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\"\n",
    "\n",
    "print(f\"Total number of characters in the file: {len(text)}\")\n",
    "\n",
    "def split_text(text, max_len=1000, overlap=200):\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < len(text):\n",
    "        end = start + max_len\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += max_len - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(text)\n",
    "print(f\"Number of chunks after splitting: {len(chunks)}\")\n",
    "print(\"Sample chunk:\\n\", chunks[0][:500])\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=API_KEY\n",
    ")\n",
    "print(\"Embeddings ready\")\n",
    "\n",
    "print(\"Computing embeddings and creating vector database...\")\n",
    "vectordb = Chroma.from_texts(chunks, embeddings)\n",
    "print(\"The vector database is ready.\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "print(\"Gemini model is ready.\")\n",
    "\n",
    "def gemini_chat(prompt: str):\n",
    "    response = model.generate_content(prompt)\n",
    "    if hasattr(response, \"text\") and response.text:\n",
    "        return response.text\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        first = response.output[0]\n",
    "        if hasattr(first, \"content\") and first.content:\n",
    "            c = first.content[0]\n",
    "            if hasattr(c, \"text\"):\n",
    "                return c.text\n",
    "        if hasattr(first, \"text\"):\n",
    "            return first.text\n",
    "    return str(response)\n",
    "\n",
    "def answer_question_with_context(question, k=3):\n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "\n",
    "    if isinstance(relevant_docs, dict):\n",
    "        if \"documents\" in relevant_docs:\n",
    "            relevant_docs = relevant_docs[\"documents\"]\n",
    "        elif \"results\" in relevant_docs:\n",
    "            relevant_docs = relevant_docs[\"results\"]\n",
    "\n",
    "    if not isinstance(relevant_docs, (list, tuple)):\n",
    "        relevant_docs = list(relevant_docs)\n",
    "\n",
    "    print(f\"\\n[INFO] {len(relevant_docs)} relevant chunks found. Using the following context:\")\n",
    "\n",
    "    context_combined = \"\"\n",
    "    used_indices = []\n",
    "    for idx, doc in enumerate(relevant_docs, 1):\n",
    "        snippet = getattr(doc, \"page_content\", str(doc)).strip().replace(\"\\n\", \" \")\n",
    "        print(f\"  Chunk {idx}: {snippet[:300]}...\")\n",
    "        context_combined += f\"\\nChunk {idx}:\\n{snippet}\\n\"\n",
    "        used_indices.append(str(idx))\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an assistant that explains the content of an agreement. \"\n",
    "        \"You have received a user question and some relevant excerpts from the document. \"\n",
    "        \"Please answer clearly and briefly, in the **same language** as the question. \"\n",
    "        \"Also mention which chunks your answer is based on.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context from the document: {context_combined}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    answer_text = gemini_chat(prompt)\n",
    "\n",
    "    attribution = f\"(Based on chunks {', '.join(used_indices)})\"\n",
    "    print(\"\\n[Gemini Answer]:\")\n",
    "    print(answer_text.strip())\n",
    "    print(attribution)\n",
    "    return answer_text\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nWhat would you like to ask about the agreement? (type 'סיום' or 'exit' to quit)\\n\")\n",
    "    if question.lower() in [\"סיום\", \"exit\", \"quit\"]:\n",
    "        print(\"Goodbye, Hope I was helpful :) \")\n",
    "        break\n",
    "    try:\n",
    "        answer_question_with_context(question)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
