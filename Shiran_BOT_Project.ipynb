{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc31b0c-1701-4d30-bb7d-45812ef9ebab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the full path to the PDF file (or press Enter to use the default):\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the file: 60092\n",
      "Number of chunks after splitting: 76\n",
      "Sample chunk:\n",
      "  \n",
      " \n",
      "1 \n",
      " \n",
      " \n",
      "AGREEMENT \n",
      " \n",
      "between \n",
      " \n",
      "The Government of the State of Israel \n",
      " \n",
      "and \n",
      " \n",
      "The Government of the United Arab Emirates \n",
      " \n",
      "on \n",
      " \n",
      "Promotion and Protection of Investments \n",
      " \n",
      "The Government of the State of Israel and The Government of The United Arab \n",
      "Emirates (hereinafter, “the Parties”) \n",
      "Further to the Treaty of Peace, Diplomatic Relations and Full Normalization between \n",
      "the United Arab Emirates and the State of Israel, signed in Washington , DC on  15 \n",
      "September 2020 (hereinafter, “the Pea\n",
      "Embeddings ready\n",
      "Computing embeddings and creating vector database...\n",
      "The vector database is ready.\n",
      "Gemini model is ready.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "What would you like to ask about the agreement? (type 'סיום' or 'exit' to quit)\n",
      " סיום\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye, Hope I was helpful :) \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pypdf import PdfReader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import GenerativeModel\n",
    "\n",
    "pdf_path = input(\"Please enter the full path to the PDF file (or press Enter to use the default):\\n\").strip()\n",
    "\n",
    "if not pdf_path:\n",
    "    pdf_path = r\"C:\\Users\\shira\\OneDrive\\שולחן העבודה\\BOT_Elad_Sistem\\international_agreements_uae_bit-eng.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    print(\"❌ File not found. Please check the path and try again.\")\n",
    "    exit(1)\n",
    "\n",
    ""API_KEY = "your-real-api-key-here" \n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\"\n",
    "\n",
    "print(f\"Total number of characters in the file: {len(text)}\")\n",
    "\n",
    "def split_text(text, max_len=1000, overlap=200):\n",
    "    start = 0\n",
    "    chunks = []\n",
    "    while start < len(text):\n",
    "        end = start + max_len\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += max_len - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(text)\n",
    "print(f\"Number of chunks after splitting: {len(chunks)}\")\n",
    "print(\"Sample chunk:\\n\", chunks[0][:500])\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=API_KEY\n",
    ")\n",
    "print(\"Embeddings ready\")\n",
    "\n",
    "print(\"Computing embeddings and creating vector database...\")\n",
    "vectordb = Chroma.from_texts(chunks, embeddings)\n",
    "print(\"The vector database is ready.\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)\n",
    "model = GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "print(\"Gemini model is ready.\")\n",
    "\n",
    "def gemini_chat(prompt: str):\n",
    "    response = model.generate_content(prompt)\n",
    "    if hasattr(response, \"text\") and response.text:\n",
    "        return response.text\n",
    "    if hasattr(response, \"output\") and response.output:\n",
    "        first = response.output[0]\n",
    "        if hasattr(first, \"content\") and first.content:\n",
    "            c = first.content[0]\n",
    "            if hasattr(c, \"text\"):\n",
    "                return c.text\n",
    "        if hasattr(first, \"text\"):\n",
    "            return first.text\n",
    "    return str(response)\n",
    "\n",
    "def answer_question_with_context(question, k=3):\n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "\n",
    "    if isinstance(relevant_docs, dict):\n",
    "        if \"documents\" in relevant_docs:\n",
    "            relevant_docs = relevant_docs[\"documents\"]\n",
    "        elif \"results\" in relevant_docs:\n",
    "            relevant_docs = relevant_docs[\"results\"]\n",
    "\n",
    "    if not isinstance(relevant_docs, (list, tuple)):\n",
    "        relevant_docs = list(relevant_docs)\n",
    "\n",
    "    print(f\"\\n[INFO] {len(relevant_docs)} relevant chunks found. Using the following context:\")\n",
    "\n",
    "    context_combined = \"\"\n",
    "    used_indices = []\n",
    "    for idx, doc in enumerate(relevant_docs, 1):\n",
    "        snippet = getattr(doc, \"page_content\", str(doc)).strip().replace(\"\\n\", \" \")\n",
    "        print(f\"  Chunk {idx}: {snippet[:300]}...\")\n",
    "        context_combined += f\"\\nChunk {idx}:\\n{snippet}\\n\"\n",
    "        used_indices.append(str(idx))\n",
    "\n",
    "    prompt = (\n",
    "        \"You are an assistant that explains the content of an agreement. \"\n",
    "        \"You have received a user question and some relevant excerpts from the document. \"\n",
    "        \"Please answer clearly and briefly, in the **same language** as the question. \"\n",
    "        \"Also mention which chunks your answer is based on.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context from the document: {context_combined}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    answer_text = gemini_chat(prompt)\n",
    "\n",
    "    attribution = f\"(Based on chunks {', '.join(used_indices)})\"\n",
    "    print(\"\\n[Gemini Answer]:\")\n",
    "    print(answer_text.strip())\n",
    "    print(attribution)\n",
    "    return answer_text\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nWhat would you like to ask about the agreement? (type 'סיום' or 'exit' to quit)\\n\")\n",
    "    if question.lower() in [\"סיום\", \"exit\", \"quit\"]:\n",
    "        print(\"Goodbye, Hope I was helpful :) \")\n",
    "        break\n",
    "    try:\n",
    "        answer_question_with_context(question)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421eda5-156f-4b06-8d59-e1b3a68e337e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
